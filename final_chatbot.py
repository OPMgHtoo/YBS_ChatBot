import json
import os
import difflib
from collections import deque
from openai import OpenAI
import re

from index_data import collection, model

# --- Local Ollama (OpenAI-compatible) ---
OLLAMA_BASE_URL = os.getenv("OPENAI_BASE_URL", "http://127.0.0.1:11434/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "qwen3:latest")

# Ollama ·Äô·Äæ·Ä¨ API key ·Äô·Äú·Ä≠·ÄØ·Äï·Ä±·Äô·Äö·Ä∑·Ä∫ OpenAI SDK ·ÄÄ field ·Äú·Ä≠·ÄØ·Äê·Äê·Ä∫·Äú·Ä≠·ÄØ·Ä∑ dummy ·Äë·Ä¨·Ä∏
client_ai = OpenAI(base_url=OLLAMA_BASE_URL, api_key=os.getenv("OPENAI_API_KEY", "ollama"))

def translate_to_english(text: str) -> str:
    prompt = (
         "Translate the following Burmese text into clear and natural English. "
         "Keep bus stop names, numbers, and special terms like YBS, YPS unchanged. "
         "Return ONLY the translation text. Do NOT add explanations, headings, or extra sentences.\n\n"
          f"{text}"
    )

    try:
        response = client_ai.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return response.choices[0].message.content.strip()
    except Exception:
        return text  # fallback


def translate_to_myanmar(text: str) -> str:
    prompt = (
        "Translate the following English text into natural Burmese language. "
        "Keep bus stop names, numbers, and special terms like YBS, YPS unchanged.\n\n"
        f"{text}"
    )

    try:
        response = client_ai.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return response.choices[0].message.content.strip()
    except Exception:
        return text  # fallback (don't break system)



def load_json_routing_data():
    stop_to_routes, route_to_stops = {}, {}
    json_path = "data/YBS_data.json"

    if not os.path.exists(json_path):
        return {}, {}, []

    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
        for line_data in data["ybs_bus_lines"]:
            route_id = f"YBS {line_data['line']}"

            stops = [" ".join(str(s).strip().replace("·ÄÖ·Äª·Ä±·Ä∏", "·Äà·Ä±·Ä∏").split()) for s in line_data["stops"]]
            route_to_stops[route_id] = stops
            for s in stops:
                if route_id not in stop_to_routes.setdefault(s, []):
                    stop_to_routes[s].append(route_id)
    return stop_to_routes, route_to_stops, list(stop_to_routes.keys())


S_TO_R, R_TO_S, MASTER_STOP_LIST = load_json_routing_data()

# (final_chatbot.py)  <-- S_TO_R, R_TO_S, MASTER_STOP_LIST = ... ·Ä°·Ä±·Ä¨·ÄÄ·Ä∫·Äê·Äî·Ä∫·Ä∏ (around line 69)

def _norm_en(s: str) -> str:
    # Han Tar Waddy  ==  HanTharWaddy  ·Äñ·Äº·ÄÖ·Ä∫·Ä°·Ä±·Ä¨·ÄÑ·Ä∫ normalize
    return re.sub(r"[^a-z0-9]", "", (s or "").lower())

EN_ALIAS_MAP = {}  # norm_en -> list of {"mm":..., "en":...}

def _load_stop_tsv_map(tsv_path="data/stops.tsv"):
    if not os.path.exists(tsv_path):
        return
    with open(tsv_path, "r", encoding="utf-8") as f:
        for row in f:
            row = row.strip()
            if not row:
                continue
            parts = row.split("\t")
            if len(parts) < 2:
                continue
            en = parts[0].strip()
            mm = parts[1].strip()
            k = _norm_en(en)
            EN_ALIAS_MAP.setdefault(k, []).append({"mm": mm, "en": en})

_load_stop_tsv_map()


def extract_entities_with_gemini(query):
    prompt = (
        "Extract the start and destination bus stop phrases from the input.\n"
        "Return ONLY JSON: {\"from\": \"...\", \"to\": \"...\"}\n"
        "Do NOT add extra keys.\n"
    )

    try:
        completion = client_ai.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "system", "content": prompt}, {"role": "user", "content": query}],
            temperature=0
        )
        return json.loads(completion.choices[0].message.content)
    except:
        return {"from": None, "to": None}


def get_best_stop_matches(name):
    if not name:
        return []

    clean_name = " ".join(
        str(name)
        .replace("·ÄÖ·Äª·Ä±·Ä∏", "·Äà·Ä±·Ä∏")
        .replace("·ÅÄ", "·Äù")
        .split()
    ).strip()

    # (final_chatbot.py) inside get_best_stop_matches() after clean_name computed (after line ~100)

    # English/Myanglish input detection (Myanmar unicode ·Äô·Äï·Ä´·Äõ·ÄÑ·Ä∫ en)
    is_en_input = not any("\u1000" <= ch <= "\u109F" for ch in clean_name)

    if is_en_input:
        qk = _norm_en(clean_name)

        hits = []
        seen = set()
        # startswith priority, then contains
        for k, lst in EN_ALIAS_MAP.items():
            if k.startswith(qk) or (qk in k):
                for item in lst:
                    mm = item["mm"]
                    if mm in MASTER_STOP_LIST and mm not in seen:
                        seen.add(mm)
                        hits.append({"name": mm, "label": item["en"]})

        # ‚úÖ HanTharWaddy() variants ·ÄÄ·Ä≠·ÄØ ·Ä°·Äõ·ÄÑ·Ä∫·Äë·ÄØ·Äê·Ä∫·ÄÅ·Äª·ÄÑ·Ä∫·Äú·Ä≠·ÄØ·Ä∑ sort
        def _rank(x):
            lk = _norm_en(x["label"])
            return (0 if lk.startswith(qk) else 1, len(lk))

        hits.sort(key=_rank)

        if hits:
            return hits[:10]

    # 1Ô∏è‚É£ Exact Burmese match (fast path)
    if clean_name in MASTER_STOP_LIST:
        return [{"name": clean_name}]

    # 2Ô∏è‚É£ Vector search in stop.tsv (stop_alias type)
    results = collection.query(
        query_embeddings=[model.encode(clean_name).tolist()],
        n_results=20,
        where={"type": "stop_alias"}
    )
    print("üîé DEBUG - Searching stop alias for:", clean_name)
    print("üîé DEBUG - Raw vector results:", results)

    # ‚úÖ 2.5) Boost: MASTER_STOP_LIST ·Äë·Ä≤·ÄÄ query ·Äï·Ä´·Äù·ÄÑ·Ä∫·Äê·Ä≤·Ä∑ stop names ·ÄÄ·Ä≠·ÄØ ·Ä°·Äõ·ÄÑ·Ä∫·Äë·Ää·Ä∑·Ä∫
    seen = set()
    output = []

    q_norm = clean_name

    # startswith first, then contains
    prefix_hits = [s for s in MASTER_STOP_LIST if s.startswith(q_norm)]
    contain_hits = [s for s in MASTER_STOP_LIST if (q_norm in s and s not in prefix_hits)]

    strict_hits = prefix_hits + contain_hits

    if strict_hits:
        print("üî• DEBUG - Strict substring hits:", strict_hits[:10])
        return [{"name": s, "label": s} for s in strict_hits[:10]]

    # limit to avoid huge list
    for s in (prefix_hits + contain_hits)[:10]:
        if s not in seen:
            seen.add(s)
            output.append({"name": s, "label": s})  # Burmese UI ·ÄÜ·Ä≠·ÄØ label=s OK (English UI ·ÄÄ·Ä≠·ÄØ later map)

    metas = results.get("metadatas", [[]])[0]
    if not metas:
        return []

    # 3Ô∏è‚É£ Canonical Burmese name only (for routing)
    seen = set()
    output = []

    for m in metas:
        mm_name = m.get("mm")
        if mm_name and mm_name in MASTER_STOP_LIST and mm_name not in seen:
            seen.add(mm_name)
            output.append({"name": mm_name, "label": m.get("en", mm_name)})

    print("üîé DEBUG - Canonical Burmese Matches:", output)

    # ‚úÖ loop ·Ä°·Äï·Äº·ÄÑ·Ä∫·Äô·Äæ·Ä¨ rerank
    q_norm = clean_name

    def rank_key(item):
        name = item.get("name", "")
        label = item.get("label", "")
        # contains = (q_norm in name) or (q_norm in label)
        length_penalty = len(name)
        starts_with = name.startswith(q_norm) or label.startswith(q_norm)
        contains = (q_norm in name) or (q_norm in label)
        return (0 if starts_with else 1 if contains else 2, length_penalty)

    output = sorted(output, key=rank_key)
    return output


def mm_stop_to_en(mm_name: str) -> str:
    if not mm_name:
        return mm_name

    q = " ".join(str(mm_name).replace("·ÄÖ·Äª·Ä±·Ä∏", "·Äà·Ä±·Ä∏").replace("·ÅÄ", "·Äù").split()).strip()

    results = collection.query(
        query_embeddings=[model.encode(q).tolist()],
        n_results=1,
        where={"type": "stop_alias"}
    )
    metas = results.get("metadatas", [[]])[0]

    print("üåç DEBUG - Converting MM -> EN:", mm_name)
    print("üåç DEBUG - EN match result:", metas)

    if metas and metas[0].get("en"):
        return metas[0]["en"]
    return mm_name  # fallback



def find_simplest_path(start_name, end_name):
    def normalize_stop(name):
        return " ".join(
            str(name).strip()
            .replace("·ÄÖ·Äª·Ä±·Ä∏", "·Äà·Ä±·Ä∏")
            .replace("·ÅÄ", "·Äù")  # ‚úÖ add this
            .split()
        )

    s_clean = normalize_stop(start_name)
    e_clean = normalize_stop(end_name)

    print("üõ£ DEBUG - Normalized Start:", s_clean)
    print("üõ£ DEBUG - Normalized End:", e_clean)

    if s_clean == e_clean: return None


    start_routes = S_TO_R.get(s_clean, [])
    end_routes = S_TO_R.get(e_clean, [])
    common_routes = set(start_routes) & set(end_routes)

    if common_routes:
        best_path = None
        min_stops = float('inf')
        print("üõ£ DEBUG - Direct Common Routes:", common_routes)

        for r_id in common_routes:
            stops = R_TO_S[r_id]

            s_indices = [i for i, x in enumerate(stops) if x == s_clean]
            e_indices = [i for i, x in enumerate(stops) if x == e_clean]

            for s_idx in s_indices:
                for e_idx in e_indices:
                    if s_idx < e_idx:
                        seg = stops[s_idx: e_idx + 1]
                        if len(seg) < min_stops:
                            min_stops = len(seg)
                            best_path = [{
                                "from": s_clean, "to": e_clean,
                                "route": r_id, "segment": seg
                            }]

                            print("üõ£ DEBUG - Final Path:", best_path)

        if best_path: return best_path


    queue = deque([(s_clean, [])])
    visited = {s_clean}

    while queue:
        curr_stop, path = queue.popleft()
        if len(path) >= 2: continue

        for r_id in S_TO_R.get(curr_stop, []):
            stops = R_TO_S[r_id]
            curr_indices = [i for i, x in enumerate(stops) if x == curr_stop]

            for c_idx in curr_indices:
                for next_stop_idx in range(c_idx + 1, len(stops)):
                    next_stop = stops[next_stop_idx]
                    if next_stop == e_clean:
                        return path + [{
                            "from": curr_stop, "to": e_clean,
                            "route": r_id, "segment": stops[c_idx: next_stop_idx + 1]
                        }]
                    if next_stop not in visited:
                        visited.add(next_stop)
                        queue.append((next_stop, path + [{
                            "from": curr_stop, "to": next_stop,
                            "route": r_id, "segment": stops[c_idx: next_stop_idx + 1]
                        }]))
                        print("üõ£ DEBUG - Transfer Path Found:", path)

    return None


def generate_smart_reply(user_query, path_data):
    if not path_data:
        return "·ÄÖ·Ä≠·Äê·Ä∫·Äô·Äõ·Äæ·Ä≠·Äï·Ä´·Äî·Ä≤·Ä∑·Äõ·Äæ·ÄÑ·Ä∫·Åä ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ ·Äõ·Äæ·Ä¨·Äô·Äê·ÄΩ·Ä±·Ä∑·Äï·Ä´·Äò·Ä∞·Ä∏·Åã"

    try:
        # ·ÄÖ·ÄÆ·Ä∏·Äõ·Äô·Ää·Ä∑·Ä∫ ·ÄÄ·Ä¨·Ä∏·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏·Äô·Äª·Ä¨·Ä∏ ·ÄÖ·Ä¨·Äõ·ÄÑ·Ä∫·Ä∏
        routes_list = list(dict.fromkeys([p['route'] for p in path_data]))
        routes_str = " ·Äî·Äæ·ÄÑ·Ä∑·Ä∫ ".join(routes_list)

        start_stop = path_data[0]['from']
        end_stop = path_data[-1]['to']

        # üîÑ Narrative Info (·ÄÄ·Ä¨·Ä∏·ÄÖ·ÄÆ·Ä∏·Äõ·Äô·Ää·Ä∑·Ä∫ ·Äï·ÄØ·Ä∂·ÄÖ·Ä∂·ÄÄ·Ä≠·ÄØ ·ÄÖ·Ä¨·Äû·Ä¨·Ä∏·Äñ·Äº·ÄÑ·Ä∑·Ä∫ ·Äñ·Ä±·Ä¨·Ä∫·Äï·Äº·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏)
        if len(path_data) == 1:
            # ·Äê·Ä≠·ÄØ·ÄÄ·Ä∫·Äõ·Ä≠·ÄØ·ÄÄ·Ä∫·Äû·ÄΩ·Ä¨·Ä∏·Äú·Ä≠·ÄØ·Ä∑·Äõ·Äê·Ä≤·Ä∑·Ä°·ÄÅ·Ä´
            p = path_data[0]
            transfer_text = f"**{p['from']}** ·Äê·ÄΩ·ÄÑ·Ä∫ **{p['route']}** ·ÄÄ·Ä≠·ÄØ ·Äê·Ä≠·ÄØ·ÄÄ·Ä∫·Äõ·Ä≠·ÄØ·ÄÄ·Ä∫·ÄÖ·ÄÆ·Ä∏·Äï·Äº·ÄÆ·Ä∏ **{p['to']}** ·Äû·Ä≠·ÄØ·Ä∑ ·Äû·ÄΩ·Ä¨·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫·Äõ·Äæ·ÄÑ·Ä∫·Åã ·Äò·Äê·Ä∫·ÄÖ·Ä∫·ÄÄ·Ä¨·Ä∏ ·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äõ·Äî·Ä∫ ·Äô·Äú·Ä≠·ÄØ·Äï·Ä´·Åã"
        else:
            # ·ÄÄ·Ä¨·Ä∏·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·Ä∏·Äõ·Äô·Ää·Ä∑·Ä∫·Ä°·ÄÅ·Ä´
            p1 = path_data[0]
            p2 = path_data[1]
            transfer_text = (
                f"**{p1['from']}** ·Äê·ÄΩ·ÄÑ·Ä∫ **{p1['route']}** ·ÄÖ·ÄÆ·Ä∏·Äï·Äº·ÄÆ·Ä∏ "
                f"**{p1['to']}** ·Äô·Äæ·Äê·Ä∫·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Äê·ÄΩ·ÄÑ·Ä∫ **{p2['route']}** ·ÄÄ·Ä¨·Ä∏·Äê·ÄÖ·Ä∫·ÄÜ·ÄÑ·Ä∑·Ä∫·Äï·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄÆ·Ä∏·ÄÄ·Ä¨ "
                f"**{p2['from']}** ·Äô·Äæ **{p2['to']}** ·Äû·Ä≠·ÄØ·Ä∑ ·Äû·ÄΩ·Ä¨·Ä∏·Äõ·Äï·Ä´·Äô·Ää·Ä∫·Åã"
            )

        # ·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ ·Äñ·Äº·Äê·Ä∫·Äû·Äî·Ä∫·Ä∏·Äõ·Äô·Ää·Ä∑·Ä∫ ·Äô·Äæ·Äê·Ä∫·Äê·Ä≠·ÄØ·ÄÑ·Ä∫·Ä°·Äõ·Ä±·Ä°·Äê·ÄΩ·ÄÄ·Ä∫ ·Äê·ÄΩ·ÄÄ·Ä∫·ÄÅ·Äª·ÄÄ·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏
        # total_stops = 0
        # for p in path_data:
        #     total_stops += len(p['segment'])
        # # ·Äï·Äº·Äî·Ä∫·Äë·Äï·Ä∫·Äî·Ä±·Äê·Ä≤·Ä∑ Transfer point ·ÄÄ·Ä≠·ÄØ ·Äî·Äæ·ÄØ·Äê·Ä∫·ÄÅ·Äº·ÄÑ·Ä∫·Ä∏
        # if len(path_data) > 1:
        #     total_stops -= 1

        return (
            f"·Äô·ÄÑ·Ä∫·Äπ·ÄÇ·Äú·Ä¨·Äï·Ä´·Äõ·Äæ·ÄÑ·Ä∫·Åã **{start_stop}** ·Äô·Äæ **{end_stop}** ·Äû·Ä≠·ÄØ·Ä∑ ·Äû·ÄΩ·Ä¨·Ä∏·Äõ·Äî·Ä∫ ·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ ·Äê·ÄΩ·Ä±·Ä∑·Äõ·Äæ·Ä≠·Äï·Ä´·Äê·Äö·Ä∫·Äõ·Äæ·ÄÑ·Ä∫·Åã\n\n"
            f"üöå **·ÄÖ·ÄÆ·Ä∏·Äî·ÄÑ·Ä∫·Ä∏·Äõ·Äô·Ää·Ä∑·Ä∫·ÄÄ·Ä¨·Ä∏:** {routes_str}  "
            f"üìç **·ÄÖ·ÄÆ·Ä∏·Äõ·Äô·Ää·Ä∑·Ä∫·Äô·Äæ·Äê·Ä∫·Äê·Ä≠·ÄØ·ÄÑ·Ä∫:** {start_stop}  "
            f"üèÅ **·ÄÜ·ÄÑ·Ä∫·Ä∏·Äõ·Äô·Ää·Ä∑·Ä∫·Äô·Äæ·Äê·Ä∫·Äê·Ä≠·ÄØ·ÄÑ·Ä∫:** {end_stop}\n\n"
            f"üîÑ **·Ä°·ÄÅ·Äª·ÄÄ·Ä∫·Ä°·Äú·ÄÄ·Ä∫:** {transfer_text}\n\n"
            # f"·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏·Äô·Äæ·Äê·Ä∫·Äê·Ä≠·ÄØ·ÄÑ·Ä∫ **({total_stops})** ·ÄÅ·ÄØ ·Äñ·Äº·Äê·Ä∫·Äû·Äî·Ä∫·Ä∏·Äû·ÄΩ·Ä¨·Ä∏·Äï·Ä´·Äô·Ää·Ä∫·Åã ·Äò·Ä±·Ä∏·ÄÄ·ÄÑ·Ä∫·Ä∏·ÄÖ·ÄΩ·Ä¨ ·Äû·ÄΩ·Ä¨·Ä∏·Äú·Ä¨·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·ÄÖ·Ä±·Äõ·Äæ·ÄÑ·Ä∫·Åã"
        )
    except Exception as e:
        return f"·Äú·Äô·Ä∫·Ä∏·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏ ·Ä°·ÄÅ·Äª·ÄÄ·Ä∫·Ä°·Äú·ÄÄ·Ä∫ ·Äë·ÄØ·Äê·Ä∫·Äï·Äº·Äî·Ä∫·Äõ·Ä¨·Äê·ÄΩ·ÄÑ·Ä∫ ·Ä°·Äô·Äæ·Ä¨·Ä∏·Ä°·Äö·ÄΩ·ÄÑ·Ä∫·Ä∏ ·Äõ·Äæ·Ä≠·Äî·Ä±·Äï·Ä´·Äû·Ää·Ä∫·Äõ·Äæ·ÄÑ·Ä∫·Åã"

def generate_smart_reply_en(path_data):
    if not path_data:
        return "Sorry, I couldn't find a route."

    routes_list = list(dict.fromkeys([p['route'] for p in path_data]))
    routes_str = " and ".join(routes_list)

    start_stop = mm_stop_to_en(path_data[0]['from'])
    end_stop = mm_stop_to_en(path_data[-1]['to'])



    if len(path_data) == 1:
        p = path_data[0]
        transfer_text = (
            f"Take **{p['route']}** directly from **{mm_stop_to_en(p['from'])}** "
            f"to **{mm_stop_to_en(p['to'])}**. No transfer is needed."
        )
    else:
        p1, p2 = path_data[0], path_data[1]
        transfer_text = (
            f"From **{mm_stop_to_en(p1['from'])}**, take **{p1['route']}** and get off at "
            f"**{mm_stop_to_en(p1['to'])}**. Then transfer to **{p2['route']}** and continue to "
            f"**{mm_stop_to_en(p2['to'])}**."
        )

    return (
        f"Route found from **{start_stop}** to **{end_stop}**.\n\n"
        f"üöå **Buses:** {routes_str}  "
        f"üìç **Board at:** {start_stop}  "
        f"üèÅ **Get off at:** {end_stop}\n\n"
        f"üîÑ **Info:** {transfer_text}"

    )



def check_greeting(text):
    text = text.lower().strip()

    # English Greetings
    en_greetings = ["hi", "hello", "hey", "good morning", "mingalabar","good night","good evening","good afternoon","sppl","chit lar"]
    # Myanmar Greetings
    mm_greetings = ["·Äô·ÄÑ·Ä∫·Äπ·ÄÇ·Äú·Ä¨·Äï·Ä´", "·Äü·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏", "·Äî·Ä±·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏·Äú·Ä¨·Ä∏", "·Äü·Äö·Ä∫·Äú·Ä≠·ÄØ"]

    if any(greet in text for greet in en_greetings):
        return "Hello! How can I help you today? Where would you like to go?"

    if any(greet in text for greet in mm_greetings):
        return "·Äô·ÄÑ·Ä∫·Äπ·ÄÇ·Äú·Ä¨·Äï·Ä´·Äõ·Äæ·ÄÑ·Ä∫·Åã ·Äí·ÄÆ·Äî·Ä±·Ä∑ ·Äò·Äö·Ä∫·ÄÄ·Ä≠·ÄØ·Äû·ÄΩ·Ä¨·Ä∏·Äñ·Ä≠·ÄØ·Ä∑ ·ÄÄ·Ä∞·Ää·ÄÆ·Äï·Ä±·Ä∏·Äõ·Äô·Äú·Ä≤·Äü·ÄÑ·Ä∫?"

    return None


def get_general_knowledge(query):
    results = collection.query(
        query_embeddings=[model.encode(query).tolist()],
        n_results=5,  # ‚úÖ 1 -> 5 (or 8)
        where={"type": "knowledge"}
    )

    cands = results["documents"][0] if results.get("documents") else []
    if not cands:
        return None

    # ‚úÖ rerank using LLM (no keyword rules)
    prompt = (
        "You are answering a user's question using ONLY the given candidate passages.\n"
        "Pick the single best passage that directly answers the question.\n"
        "Return ONLY the chosen passage text, no explanations.\n\n"
        f"Question:\n{query}\n\n"
        "Candidates:\n" + "\n\n---\n\n".join(cands)
    )

    resp = client_ai.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    return resp.choices[0].message.content.strip()



def classify_intent_with_gemini(user_input):
    prompt = f"""
    You are an AI classifier for a Yangon Bus Chatbot. 
    Classify the following user input into one of these 4 categories:
    1. 'greeting' : If user says hi, hello, or asks how are you.
    2. 'bus_route' : If user asks how to go from one place to another (must have both source and destination).
    3. 'knowledge' : If user asks about YBS history, system rules, YPS card details, number of buses, or general facts.
    4. 'single_stop' : If user mentions only one bus stop name without asking for a route or knowledge.

    User input: "{user_input}"

    Return ONLY the category name in lowercase.
    """

    try:
        response = client_ai.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        return response.choices[0].message.content.strip().lower()
    except:
        return "knowledge"  # Error ·Äê·ÄÄ·Ä∫·Äõ·ÄÑ·Ä∫ default knowledge ·Äú·Ä≠·ÄØ·Ä∑·Äë·Ä¨·Ä∏·Äî·Ä≠·ÄØ·ÄÑ·Ä∫·Äï·Ä´·Äê·Äö·Ä∫

# --- REPLACE THIS FUNCTION in final_chatbot.py ---


# def get_general_knowledge(query):
#     txt_path = "data/New Text Document.txt"  # index_data.py ·Äë·Ä≤·ÄÄ path ·Äî·Ä≤·Ä∑·Äê·Ä∞·Ä°·Ä±·Ä¨·ÄÑ·Ä∫·Äë·Ä¨·Ä∏·Äï·Ä´  ‚úÖ
#
#     if not os.path.exists(txt_path):
#         return None
#
#     with open(txt_path, "r", encoding="utf-8") as f:
#         content = f.read()
#
#     # Paragraph-wise split (·Äû·ÄÑ·Ä∑·Ä∫ index_data.py ·Äî·Ä≤·Ä∑·Äê·Ä∞·Äê·Ä≤·Ä∑ split pattern)
#     paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
#
#     q = query.replace(" ", "")
#     is_model_q = ("model" in query.lower()) or ("·Äò·Äê·Ä∫·ÄÖ·Ä∫·ÄÄ·Ä¨·Ä∏" in query) or ("·Äô·Ä±·Ä¨·Ä∫·Äí·Äö·Ä∫" in query)
#
#     if is_model_q:
#         model_terms = ["Yutong", "Ankai", "Foton", "Mini Bus", "·Äô·ÄÆ·Äî·ÄÆ·Äò·Äê·Ä∫·ÄÖ·Ä∫", "·Äê·Ä±·Ä¨·ÄÑ·Ä∫·ÄÄ·Ä≠·ÄØ·Äõ·ÄÆ·Ä∏·Äö·Ä¨·Ä∏", "·Äê·ÄÖ·Ä∫·Äï·Äê·Ä∫·Äõ·ÄÖ·Ä∫", "·Äê·Äõ·ÄØ·Äê·Ä∫"]
#         for p in paragraphs:
#             if any(t in p for t in model_terms):
#                 return p
#
#     # (1) Count-type question (e.g., "·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏ ·Äò·Äö·Ä∫·Äî·Äæ·ÄÖ·Ä∫·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏") ‚Üí ·ÅÇ·ÅÄ·ÅÇ·ÅÑ/·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏/·Äí·Ä±·Äê·Ä¨ ·Äï·Ä´·Äê·Ä≤·Ä∑ paragraph ·ÄÄ·Ä≠·ÄØ priority
#     # --- ADD inside get_general_knowledge() ---
#
#     q_lower = query.lower()
#
#     if "ybs card" in q_lower:
#         query = query + " YPS Card"
#         q_lower = query.lower()
#
#     is_total_lines_q_en = (
#             ("how many" in q_lower or "number of" in q_lower)
#             and ("line" in q_lower or "lines" in q_lower or "routes" in q_lower)
#             and ("ybs" in q_lower)
#     )
#
#     is_total_lines_q_mm = (
#             ("·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏" in q or "·ÄÄ·Ä¨·Ä∏·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏" in q or "·Äö·Ä¨·Äâ·Ä∫·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏" in q)
#             and ("·Äò·Äö·Ä∫·Äî·Äæ" in q or "·Äò·Äö·Äπ·Çè·ÄΩ" in q or "·ÄÖ·ÄØ·ÄÖ·ÄØ·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏" in q)
#     )
#
#     if is_total_lines_q_en or is_total_lines_q_mm:
#         # 2024 + line paragraph priority
#         for p in paragraphs:
#             if ("·ÅÇ·ÅÄ·ÅÇ·ÅÑ" in p and "·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏" in p and "·Äí·Ä±·Äê·Ä¨" in p):
#                 return p
#         for p in paragraphs:
#             if ("·ÅÇ·ÅÄ·ÅÇ·ÅÑ" in p and "·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏" in p):
#                 return p
#         for p in paragraphs:
#             if ("·ÅÇ·ÅÄ·ÅÇ·ÅÑ" in p) and (("132" in p) or ("·ÅÅ·ÅÉ·ÅÇ" in p)):
#                 return p
#
#     # (2) Normal knowledge fallback: keyword overlap ·Äî·Ä≤·Ä∑ best paragraph ·Äö·Ä∞
#     keywords = [k for k in ["YBS", "YRTA", "YPS", "Card", "·ÄÄ·Äê·Ä∫", "·Äö·Ä¨·Äâ·Ä∫·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏", "·Äú·Ä≠·ÄØ·ÄÑ·Ä∫·Ä∏", "·Äí·Ä±·Äê·Ä¨", "·ÅÇ·ÅÄ·ÅÇ·ÅÑ",
#                             "how many", "number", "line", "lines", "route", "routes", "exist"] if k in query or k in q]
#     best_p, best_score = None, -1
#     for p in paragraphs:
#         score = sum(1 for k in keywords if k in p)
#         if score > best_score:
#             best_score = score
#             best_p = p
#
#     return best_p if best_score > 0 else None


