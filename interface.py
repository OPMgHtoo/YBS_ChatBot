import streamlit as st
import json
import time
import ollama

from final_chatbot import (
    generate_smart_reply,
    find_simplest_path,
    get_best_stop_matches, YBS_STATS
)


context = {
    "stats": YBS_STATS,
    "found_routes": [],
    "ambiguous_stops": {}
}
st.set_page_config(page_title="YBS Chatbot", page_icon="ğŸšŒ")

st.title("ğŸšŒ Yangon Bus Chatbot")
st.caption("á€›á€”á€ºá€€á€¯á€”á€ºá€™á€¼á€­á€¯á€·á€á€½á€„á€ºá€¸ á€á€›á€®á€¸á€á€½á€¬á€¸á€œá€¬á€›á€±á€¸á€¡á€á€½á€€á€º á€œá€™á€ºá€¸á€Šá€½á€¾á€”á€ºá€•á€±á€¸á€™á€Šá€·á€º AI á€œá€€á€ºá€‘á€±á€¬á€€á€º")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# if prompt := st.chat_input("á€˜á€šá€ºá€€á€”á€± á€˜á€šá€ºá€€á€­á€¯ á€á€½á€¬á€¸á€á€»á€„á€ºá€•á€«á€á€œá€²?"):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)
#
#     with st.chat_message("assistant"):
#         message_placeholder = st.empty()
#         message_placeholder.markdown("ğŸ” á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸ á€…á€…á€ºá€†á€±á€¸á€”á€±á€•á€«á€á€šá€º...")
#
#         try:
#             # á‚á‹ Entity Extraction (Call 1)
#             extract_prompt = f"""
#             Extract 'from' and 'to' bus stops from the text.
#             Text: "{prompt}"
#             Return JSON ONLY: {{"from": "string or null", "to": "string or null"}}
#             """
#             extract_res = ollama.chat(
#                 model="qwen3:latest",
#                 messages=[{"role": "user", "content": extract_prompt}],
#                 format="json"
#             )
#             entities = json.loads(extract_res["message"]["content"])
#             from_name = entities.get("from")
#             to_name = entities.get("to")
#
#             # áƒá‹ Backend Logic (Matching & Routing) á€€á€­á€¯ á€á€»á€­á€á€ºá€†á€€á€ºá€á€¼á€„á€ºá€¸
#             context = {"found_routes": [], "ambiguous_stops": {}}
#
#             # á€™á€¾á€á€ºá€á€­á€¯á€„á€ºá€¡á€™á€Šá€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€á€¼á€„á€ºá€¸
#             starts = get_best_stop_matches(from_name)
#             ends = get_best_stop_matches(to_name)
#
#             if starts and ends:
#                 # find_simplest_path function á€€á€­á€¯ á€á€±á€«á€ºá€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸
#                 path = find_simplest_path(starts[0]["name"], ends[0]["name"])
#                 if path:
#                     context["found_routes"] = path
#                 else:
#                     if len(starts) > 1: context["ambiguous_stops"]["from"] = starts
#                     if len(ends) > 1: context["ambiguous_stops"]["to"] = ends
#
#             # á„á‹ Final Smart Reply (Call 2)
#             full_response = generate_smart_reply(prompt, context)
#
#             message_placeholder.markdown(full_response)
#             st.session_state.messages.append({"role": "assistant", "content": full_response})
#
#         except Exception as e:
#             message_placeholder.markdown("á€…á€”á€…á€ºá€¡á€á€½á€„á€ºá€¸ á€¡á€™á€¾á€¬á€¸á€¡á€šá€½á€„á€ºá€¸á€á€…á€ºá€á€¯ á€–á€¼á€…á€ºá€•á€±á€«á€ºá€á€²á€·á€•á€«á€á€šá€ºá€›á€¾á€„á€ºá‹")
#             st.error(f"Error Details: {e}")

if prompt := st.chat_input("á€˜á€šá€ºá€€á€”á€± á€˜á€šá€ºá€€á€­á€¯ á€á€½á€¬á€¸á€á€»á€„á€ºá€•á€«á€á€œá€²?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        status_text = st.empty()  # Timer

        start_time = time.time()  # Timer

        try:
            # áá‹ Entity Extraction (Call 1)
            extract_prompt = f"""
            Extract 'from' and 'to' bus stops from the text.
            Text: "{prompt}"
            Return JSON ONLY: {{"from": "string or null", "to": "string or null"}}
            """

            try:
                extract_res = ollama.chat(
                    model="qwen3:latest",
                    messages=[{"role": "user", "content": extract_prompt}],
                    format="json"
                )
                entities = json.loads(extract_res["message"]["content"])
                from_name = entities.get("from")
                to_name = entities.get("to")
            except Exception as e:
                from_name, to_name = None, None
                print(f"Error extracting entities: {e}")

            # á‚á‹ Logic & Routing


            context = {"found_routes": [], "ambiguous_stops": {}}
            starts = get_best_stop_matches(from_name)
            ends = get_best_stop_matches(to_name)

            if starts and ends:
                path = find_simplest_path(starts[0]["name"], ends[0]["name"])
                if path:
                    context["found_routes"] = path
                else:
                    if len(starts) > 1: context["ambiguous_stops"]["from"] = starts
                    if len(ends) > 1: context["ambiguous_stops"]["to"] = ends

            # áƒá‹ Final Reply Generation (Call 2)
            full_response = generate_smart_reply(prompt, context)

            # á„á‹ Timer
            end_time = time.time()
            total_time = end_time - start_time

            message_placeholder.markdown(full_response)
            status_text.markdown(f"â±ï¸ Response time: {total_time:.2f} seconds")  # á€¡á€±á€¬á€€á€ºá€á€¼á€±á€™á€¾á€¬ á€¡á€á€»á€­á€”á€ºá€•á€¼á€™á€šá€º

            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            status_text.empty()
            message_placeholder.markdown("á€…á€”á€…á€ºá€¡á€á€½á€„á€ºá€¸ á€¡á€™á€¾á€¬á€¸á€¡á€šá€½á€„á€ºá€¸á€á€…á€ºá€á€¯ á€–á€¼á€…á€ºá€•á€±á€«á€ºá€á€²á€·á€•á€«á€á€šá€ºá€›á€¾á€„á€ºá‹")
            st.error(f"Error Details: {e}")